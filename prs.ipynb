{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code follows the same steps to test PRS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    matthews_corrcoef, \n",
    "    accuracy_score,\n",
    "    classification_report, \n",
    "    confusion_matrix,\n",
    "    roc_curve, \n",
    "    precision_recall_curve, \n",
    "    auc\n",
    ")\n",
    "\n",
    "# Imbalanced-learn imports\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_filtered_data():\n",
    "    \"\"\"Load data with explicit dtype specification\"\"\"\n",
    "    # Load with explicit dtype for problematic columns\n",
    "    df = pd.read_csv(\"data_participant.csv\", low_memory=False)\n",
    "    demo = pd.read_csv(\"data_year_of_birth_and_ethnicity.csv\", low_memory=False)\n",
    "    prs = pd.read_csv(\"prs.csv\")\n",
    "    \n",
    "    # Rest of your existing function...\n",
    "    \n",
    "    # Ensure consistent ID format\n",
    "    df['eid'] = df['eid'].astype(str).str.strip()\n",
    "    demo['eid'] = demo['eid'].astype(str).str.strip()\n",
    "    prs['eid'] = prs['eid'].astype(str).str.strip()\n",
    "    \n",
    "    # Merge datasets\n",
    "    df = df.merge(\n",
    "        demo[['eid', 'p21022', 'p31', 'p40001_i1', 'p40006_i0', 'p40008_i0']],\n",
    "        on='eid',\n",
    "        how='left'\n",
    "    ).merge(\n",
    "        prs,  # Merge with PRS data\n",
    "        on='eid',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Apply filters\n",
    "    df = df[\n",
    "        (df['p31'] != 'Female') &                # Only males\n",
    "        (df['p21022'] > 50) &                    # Age at recruitment >50\n",
    "        ~(                                       # Exclude prostate cancer ≤50\n",
    "            (df['p40006_i0'].astype(str).str.contains(\"C61\", na=False)) & \n",
    "            (df['p40008_i0'] <= 50)\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    # Create target\n",
    "    df['target'] = (\n",
    "        df['p40001_i0'].str.contains('C61', na=False) |\n",
    "        df['p40001_i1'].str.contains('C61', na=False)\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Remove leakage features\n",
    "    leakage_features = [\n",
    "        'p40000_i0', 'p40005_i8', 'p40006_i0', 'p40008_i0',\n",
    "        *[col for col in df.columns if 'date_of_death' in col.lower()]\n",
    "    ]\n",
    "    df = df.drop(columns=[col for col in leakage_features if col in df.columns])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==================== FEATURE ENGINEERING ====================\n",
    "def extract_all_features(df):\n",
    "    \"\"\"Feature extraction focusing on specific features and PRS columns\"\"\"\n",
    "    # 1. List of your specified features (without _dur)\n",
    "    specified_features = [\n",
    "        'p30850_i0', 'p30720_i0', 'p30700_i0', \n",
    "        'p30630_i0', 'p30610_i0', 'p48_i0', \n",
    "        'p30830_i0', 'p21002_i0'\n",
    "    ]\n",
    "    \n",
    "    # 2. Your PRS columns (add your specific PRS columns here)\n",
    "    prs_cols = ['p26267', 'p26268']  # ← REPLACE WITH YOUR ACTUAL PRS COLUMNS\n",
    "    \n",
    "    # 3. Other standard features you want to keep\n",
    "    numerical = ['p21001_i0', 'p50_i0', 'p47_i0']  # example - adjust as needed\n",
    "    categorical = ['p20116_i0', 'p1618_i0', 'p3637_i0']  # example - adjust as needed\n",
    "    \n",
    "    # Create preprocessing pipeline\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "    \n",
    "    # Combine all numeric features\n",
    "    all_numeric = specified_features + numerical + prs_cols\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, all_numeric),\n",
    "        ('cat', categorical_transformer, categorical)\n",
    "    ])\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names\n",
    "    categorical_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical)\n",
    "    feature_names = all_numeric + list(categorical_features)\n",
    "    \n",
    "    return X_processed, y, feature_names\n",
    "\n",
    "\n",
    "def training_split(x, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the data into train and test sets, ensuring no NaNs in y.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: pd.DataFrame or np.ndarray of features\n",
    "    - y: pd.Series or np.ndarray of labels\n",
    "    - test_size: proportion of data to use as test set\n",
    "    - random_state: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    mask = ~y.isna()\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        x, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def retrain_logreg_top_features_test(\n",
    "    X_train, y_train, X_test, y_test, feature_names,\n",
    "    specified_features=None,\n",
    "    prs_features=['p26267', 'p26268'],\n",
    "    n_features=24,\n",
    "    output_dir=\"output\",\n",
    "    eids_test=None,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete and fixed logistic regression training with:\n",
    "    - Feature selection\n",
    "    - SMOTE balancing\n",
    "    - Comprehensive evaluation\n",
    "    - PRS analysis\n",
    "    - High-quality visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Feature Selection\n",
    "    if specified_features is None:\n",
    "        specified_features = [\n",
    "            'p30850_i0', 'p30720_i0', 'p30700_i0',\n",
    "            'p30630_i0', 'p30610_i0', 'p48_i0',\n",
    "            'p30830_i0', 'p21002_i0'\n",
    "        ]\n",
    "    \n",
    "    # Get available features\n",
    "    available_features = [f for f in specified_features + prs_features if f in feature_names]\n",
    "    selected_features = available_features[:n_features]\n",
    "    print(f\"Using {len(selected_features)} features including {len([f for f in selected_features if f in prs_features])} PRS features\")\n",
    "    \n",
    "    # 2. Data Preparation\n",
    "    if isinstance(X_train, np.ndarray):\n",
    "        feat_idx = [feature_names.index(f) for f in selected_features]\n",
    "        X_train_sub = X_train[:, feat_idx]\n",
    "        X_test_sub = X_test[:, feat_idx]\n",
    "    else:\n",
    "        X_train_sub = X_train[selected_features]\n",
    "        X_test_sub = X_test[selected_features]\n",
    "    \n",
    "    # 3. Apply SMOTE\n",
    "    sm = SMOTE(random_state=random_state)\n",
    "    X_bal, y_bal = sm.fit_resample(X_train_sub, y_train)\n",
    "    print(f\"Class balance after SMOTE: {sum(y_bal)} positives, {len(y_bal)-sum(y_bal)} negatives\")\n",
    "    \n",
    "    # 4. Model Training\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_bal, y_bal)\n",
    "    \n",
    "    # ======================\n",
    "    # 5. Plotting Functions\n",
    "    # ======================\n",
    "    \n",
    "    def plot_combined_curves(y_true, probs, split_name):\n",
    "        \"\"\"Plot ROC and PR curves side by side\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # ROC Curve\n",
    "        plt.subplot(1, 2, 1)\n",
    "        fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "        roc_auc = roc_auc_score(y_true, probs)\n",
    "        plt.plot(fpr, tpr, color='blue', lw=2, \n",
    "                label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{split_name} ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        \n",
    "        # PR Curve\n",
    "        plt.subplot(1, 2, 2)\n",
    "        precision, recall, _ = precision_recall_curve(y_true, probs)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, color='green', lw=2,\n",
    "                label=f'PR (AUC = {pr_auc:.2f})')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'{split_name} Precision-Recall Curve')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{split_name}_curves.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_importance(coefs, features):\n",
    "        \"\"\"Plot horizontal bar chart of feature coefficients\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        feat_imp = pd.DataFrame({\n",
    "            'Feature': features, \n",
    "            'Coefficient': coefs,\n",
    "            'Absolute': np.abs(coefs)\n",
    "        }).sort_values('Absolute', ascending=True)\n",
    "        \n",
    "        colors = ['red' if x in prs_features else 'blue' for x in feat_imp.Feature]\n",
    "        plt.barh(feat_imp['Feature'], feat_imp['Coefficient'], color=colors)\n",
    "        plt.title('Feature Coefficients (PRS in Red)')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/feature_importance.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_confusion_matrix(y_true, preds, split_name, metrics):\n",
    "        \"\"\"Plot annotated confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(y_true, preds)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'{split_name} Confusion Matrix\\nAUC: {metrics[\"auc\"]:.2f} | MCC: {metrics[\"mcc\"]:.2f}')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/{split_name}_confusion_matrix.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 6. Generate Predictions and Plots\n",
    "    results = {}\n",
    "    for split_name, X, y in [('Train', X_train_sub, y_train), \n",
    "                            ('Test', X_test_sub, y_test)]:\n",
    "        probs = model.predict_proba(X)[:, 1]\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        split_results = {\n",
    "            'auc': roc_auc_score(y, probs),\n",
    "            'mcc': matthews_corrcoef(y, preds),\n",
    "            'accuracy': accuracy_score(y, preds),\n",
    "            'report': classification_report(y, preds, output_dict=True)\n",
    "        }\n",
    "        results[split_name.lower()] = split_results\n",
    "        \n",
    "        # Generate plots\n",
    "        plot_combined_curves(y, probs, split_name)\n",
    "        plot_confusion_matrix(y, preds, split_name, split_results)\n",
    "    \n",
    "    # Feature Importance Plot\n",
    "    plot_feature_importance(model.coef_[0], selected_features)\n",
    "    \n",
    "    # 7. PRS Analysis\n",
    "    prs_results = {}\n",
    "    if prs_features:\n",
    "        for prs in prs_features:\n",
    "            if prs in selected_features:\n",
    "                idx = selected_features.index(prs)\n",
    "                prs_results[prs] = {\n",
    "                    'coefficient': model.coef_[0][idx],\n",
    "                    'abs_importance': abs(model.coef_[0][idx]),\n",
    "                    'rank': np.argsort(np.abs(model.coef_[0]))[::-1].tolist().index(idx) + 1\n",
    "                }\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'features': selected_features,\n",
    "        'results': results,\n",
    "        'prs_analysis': prs_results,\n",
    "        'plots': {\n",
    "            'roc_pr_curves': f\"{output_dir}/Test_curves.png\",\n",
    "            'confusion_matrix': f\"{output_dir}/Test_confusion_matrix.png\",\n",
    "            'feature_importance': f\"{output_dir}/feature_importance.png\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Extracting features...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "df = load_filtered_data()\n",
    "\n",
    "\n",
    "# 2. Extract features and target\n",
    "print(\"Extracting features...\")\n",
    "X, y, feature_names = extract_all_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/test sets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 features including 2 PRS features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\bioinfo\\fob\\pymol\\Newfolder\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance after SMOTE: 134367 positives, 134367 negatives\n",
      "Test AUC: 0.6840155638149599\n",
      "Train MCC: 0.05251007906199221\n",
      "PRS coefficients: {'p26267': {'coefficient': 0.668440373141921, 'abs_importance': 0.668440373141921, 'rank': 1}, 'p26268': {'coefficient': -0.05174845566029922, 'abs_importance': 0.05174845566029922, 'rank': 4}}\n",
      "\n",
      "Full test results:\n",
      "auc: 0.6840155638149599\n",
      "mcc: 0.05251007906199221\n",
      "accuracy: 0.6407941705755672\n",
      "report: {'0': {'precision': 0.9947329513953058, 'recall': 0.6409264110502501, 'f1-score': 0.7795640524295749, 'support': 33592.0}, '1': {'precision': 0.015588019260589243, 'recall': 0.6262295081967213, 'f1-score': 0.030418856505813028, 'support': 305.0}, 'accuracy': 0.6407941705755672, 'macro avg': {'precision': 0.5051604853279476, 'recall': 0.6335779596234856, 'f1-score': 0.40499145446769397, 'support': 33897.0}, 'weighted avg': {'precision': 0.9859227556758884, 'recall': 0.6407941705755672, 'f1-score': 0.7728233590125543, 'support': 33897.0}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting data into train/test sets...\")\n",
    "X_train, X_test, y_train, y_test = training_split(X, y)\n",
    "\n",
    "results = retrain_logreg_top_features_test(\n",
    "    X_train, y_train, X_test, y_test, feature_names,\n",
    "    specified_features=['p30850_i0', 'p30720_i0', ...],  # Your features\n",
    "    prs_features=['p26267', 'p26268'],  # Your PRS columns\n",
    "    output_dir=\"./my_results\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Test AUC: {results['results']['test']['auc']}\")\n",
    "print(f\"Train MCC: {results['results']['test']['mcc']}\")\n",
    "print(f\"PRS coefficients: {results['prs_analysis']}\")\n",
    "\n",
    "# To see all available metrics:\n",
    "print(\"\\nFull test results:\")\n",
    "for metric, value in results['results']['test'].items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
